# ============================================================================
# DEMOSTRACI√ìN DE REDES NEURONALES CONVOLUCIONALES CON CIFAR-10
# Programa educativo optimizado para entrenamiento r√°pido
# ============================================================================

# Importaci√≥n de las bibliotecas necesarias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import time
import warnings
warnings.filterwarnings('ignore')

# Configurar TensorFlow para mejor rendimiento
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("GPU disponible para aceleraci√≥n")
else:
    print("Ejecutando en CPU")

print("TensorFlow versi√≥n:", tf.__version__)
print("¬øGPU disponible?", tf.config.list_physical_devices('GPU'))

# ============================================================================
# PASO 1: CARGA Y EXPLORACI√ìN DE LOS DATOS
# ============================================================================

print("\n" + "="*60)
print("PASO 1: CARGANDO Y EXPLORANDO LOS DATOS CIFAR-10")
print("="*60)

# CIFAR-10 contiene 60,000 im√°genes de 32x32 en 10 clases
# Las clases son: avi√≥n, autom√≥vil, p√°jaro, gato, ciervo, perro, rana, caballo, barco, cami√≥n
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Nombres de las clases para hacer m√°s interpretables los resultados
nombres_clases = ['Avi√≥n', 'Autom√≥vil', 'P√°jaro', 'Gato', 'Ciervo', 
                  'Perro', 'Rana', 'Caballo', 'Barco', 'Cami√≥n']

print(f"Forma de datos de entrenamiento: {x_train.shape}")
print(f"Forma de etiquetas de entrenamiento: {y_train.shape}")
print(f"Forma de datos de prueba: {x_test.shape}")
print(f"Forma de etiquetas de prueba: {y_test.shape}")
print(f"N√∫mero de clases: {len(nombres_clases)}")

# Visualizaci√≥n de algunas im√°genes de ejemplo (opcional - comentar para acelerar)
"""
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
fig.suptitle('Ejemplos de im√°genes CIFAR-10', fontsize=16)

for i in range(10):
    fila = i // 5
    col = i % 5
    axes[fila, col].imshow(x_train[i])
    axes[fila, col].set_title(f'{nombres_clases[y_train[i][0]]}')
    axes[fila, col].axis('off')

plt.tight_layout()
plt.show()
"""

# ============================================================================
# PASO 2: PREPROCESAMIENTO DE DATOS
# ============================================================================

print("\n" + "="*60)
print("PASO 2: PREPROCESAMIENTO DE LOS DATOS")
print("="*60)

# Normalizaci√≥n de p√≠xeles: convertir de rango [0, 255] a [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

print("Datos normalizados al rango [0, 1]")

# Conversi√≥n de etiquetas a codificaci√≥n categ√≥rica (one-hot encoding)
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

print(f"Etiquetas convertidas a formato one-hot")

# Configurar tama√±o de batch para equilibrar velocidad y memoria
BATCH_SIZE = 128  # Aumentado para acelerar entrenamiento
VALIDATION_SPLIT = 0.2

# ============================================================================
# PASO 3: CONSTRUCCI√ìN DEL MODELO CNN
# ============================================================================

print("\n" + "="*60)
print("PASO 3: CONSTRUYENDO LA RED NEURONAL CONVOLUCIONAL OPTIMIZADA")
print("="*60)

def crear_modelo_cnn_eficiente():
    """
    Construye una CNN optimizada para velocidad y rendimiento
    """
    modelo = keras.Sequential([
        # Primera capa convolucional - entrada
        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        
        # Segunda capa convolucional
        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        layers.MaxPooling2D((2, 2)),
        
        # Tercera capa convolucional
        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        layers.MaxPooling2D((2, 2)),
        
        # Capa de clasificaci√≥n
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.4),  # Solo un dropout al final
        layers.Dense(10, activation='softmax')  # 10 clases
    ])
    
    return modelo

# Crear el modelo
modelo = crear_modelo_cnn_eficiente()

# Mostrar resumen del modelo
print("Arquitectura del modelo optimizado:")
modelo.summary()

# ============================================================================
# PASO 4: COMPILACI√ìN DEL MODELO
# ============================================================================

print("\n" + "="*60)
print("PASO 4: COMPILANDO EL MODELO")
print("="*60)

# Configuraci√≥n del optimizador
modelo.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Learning rate est√°ndar para equilibrio
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("Modelo compilado exitosamente!")
print("Optimizador: Adam (learning_rate=0.001)")
print("Funci√≥n de p√©rdida: categorical_crossentropy")
print("M√©tricas: accuracy")

# ============================================================================
# PASO 5: CONFIGURACI√ìN DE CALLBACKS
# ============================================================================

print("\n" + "="*60)
print("PASO 5: CONFIGURANDO CALLBACKS PARA ENTRENAMIENTO EFICIENTE")
print("="*60)

# Callback para reducir la tasa de aprendizaje cuando el rendimiento se estanca
reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=3,
    min_lr=0.0001,
    verbose=1
)

# Callback para detener el entrenamiento temprano
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

# Lista de callbacks
callbacks = [reduce_lr, early_stopping]

print("Callbacks configurados:")
print("- ReduceLROnPlateau: reduce tasa de aprendizaje cuando la precisi√≥n se estanca")
print("- EarlyStopping: detiene el entrenamiento cuando no hay m√°s mejoras")

# ============================================================================
# PASO 6: ENTRENAMIENTO DEL MODELO
# ============================================================================

print("\n" + "="*60)
print("PASO 6: ENTRENANDO EL MODELO")
print("="*60)

# Configuraci√≥n del entrenamiento
EPOCHS = 25  # M√°ximo de √©pocas

print(f"Configuraci√≥n del entrenamiento:")
print(f"- √âpocas m√°ximas: {EPOCHS}")
print(f"- Tama√±o del lote: {BATCH_SIZE}")
print(f"- Divisi√≥n de validaci√≥n: {VALIDATION_SPLIT}")

print("\nIniciando entrenamiento...")
start_time = time.time()

# Entrenar el modelo
history = modelo.fit(
    x_train, y_train_cat,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=VALIDATION_SPLIT,
    callbacks=callbacks,
    verbose=1
)

end_time = time.time()
training_time = end_time - start_time

print("\n¬°Entrenamiento completado!")
print(f"Tiempo total de entrenamiento: {training_time:.2f} segundos")
print(f"Tiempo promedio por √©poca: {training_time / len(history.history['loss']):.2f} segundos")

# ============================================================================
# PASO 7: VISUALIZACI√ìN DEL ENTRENAMIENTO
# ============================================================================

print("\n" + "="*60)
print("PASO 7: ANALIZANDO EL PROGRESO DEL ENTRENAMIENTO")
print("="*60)

def plot_training_history(history):
    """Visualiza las m√©tricas de entrenamiento a lo largo de las √©pocas"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Gr√°fico de precisi√≥n
    ax1.plot(history.history['accuracy'], label='Precisi√≥n de entrenamiento', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Precisi√≥n de validaci√≥n', linewidth=2)
    ax1.set_title('Evoluci√≥n de la Precisi√≥n del Modelo', fontsize=14)
    ax1.set_xlabel('√âpoca')
    ax1.set_ylabel('Precisi√≥n')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Gr√°fico de p√©rdida
    ax2.plot(history.history['loss'], label='P√©rdida de entrenamiento', linewidth=2)
    ax2.plot(history.history['val_loss'], label='P√©rdida de validaci√≥n', linewidth=2)
    ax2.set_title('Evoluci√≥n de la P√©rdida del Modelo', fontsize=14)
    ax2.set_xlabel('√âpoca')
    ax2.set_ylabel('P√©rdida')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# Mostrar gr√°ficos de entrenamiento
plot_training_history(history)

# Estad√≠sticas finales del entrenamiento
final_train_acc = history.history['accuracy'][-1]
final_val_acc = history.history['val_accuracy'][-1]

print(f"Resultados finales del entrenamiento:")
print(f"- Precisi√≥n de entrenamiento: {final_train_acc:.4f}")
print(f"- Precisi√≥n de validaci√≥n: {final_val_acc:.4f}")

# ============================================================================
# PASO 8: EVALUACI√ìN EN DATOS DE PRUEBA
# ============================================================================

print("\n" + "="*60)
print("PASO 8: EVALUACI√ìN FINAL EN DATOS DE PRUEBA")
print("="*60)

# Evaluar el modelo en el conjunto de prueba
test_loss, test_accuracy = modelo.evaluate(x_test, y_test_cat, verbose=0)
print(f"Precisi√≥n en datos de prueba: {test_accuracy:.4f}")
print(f"P√©rdida en datos de prueba: {test_loss:.4f}")

# Generar predicciones para an√°lisis detallado
y_pred_probs = modelo.predict(x_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test_cat, axis=1)

# Reporte de clasificaci√≥n detallado
print("\nReporte de clasificaci√≥n detallado:")
print(classification_report(y_true, y_pred, 
                          target_names=nombres_clases, digits=4))

# ============================================================================
# PASO 9: MATRIZ DE CONFUSI√ìN
# ============================================================================

print("\n" + "="*60)
print("PASO 9: AN√ÅLISIS DE ERRORES CON MATRIZ DE CONFUSI√ìN")
print("="*60)

# Calcular matriz de confusi√≥n
cm = confusion_matrix(y_true, y_pred)

# Visualizar matriz de confusi√≥n
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=nombres_clases, yticklabels=nombres_clases)
plt.title('Matriz de Confusi√≥n - Resultados de Clasificaci√≥n', fontsize=16)
plt.xlabel('Predicciones')
plt.ylabel('Valores Reales')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# An√°lisis de precisi√≥n por clase
precisiones_por_clase = cm.diagonal() / cm.sum(axis=1)
print("Precisi√≥n por clase:")
for i, nombre in enumerate(nombres_clases):
    print(f"{nombre}: {precisiones_por_clase[i]:.4f}")

# ============================================================================
# PASO 10: RESUMEN Y CONCLUSIONES
# ============================================================================

print("\n" + "="*60)
print("PASO 10: RESUMEN Y CONCLUSIONES")
print("="*60)

print("üéâ ¬°DEMOSTRACI√ìN COMPLETADA EXITOSAMENTE!")
print("\nResumen de resultados del modelo optimizado:")
print(f"‚úÖ Precisi√≥n final en datos de prueba: {test_accuracy:.1%}")
print(f"‚úÖ Tiempo total de entrenamiento: {training_time:.2f} segundos")
print(f"‚úÖ Tiempo promedio por √©poca: {training_time / len(history.history['loss']):.2f} segundos")
print(f"‚úÖ N√∫mero de par√°metros del modelo: {modelo.count_params():,}")
print(f"‚úÖ Entrenamiento completado en {len(history.history['loss'])} √©pocas")

print("\nOptimizaciones aplicadas:")
print("‚Ä¢ Arquitectura CNN m√°s eficiente")
print("‚Ä¢ Batch size aumentado a 128")
print("‚Ä¢ Menos capas de regularizaci√≥n")
print("‚Ä¢ Estrategias eficientes de entrenamiento")

print("\n" + "="*60)
print("¬°Gracias por explorar las redes neuronales convolucionales!")
print("="*60)