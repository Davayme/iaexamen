# ============================================================================
# DEMOSTRACI√ìN DE REDES NEURONALES CONVOLUCIONALES CON CIFAR-10
# Programa educativo optimizado para reducir overfitting y mantener velocidad
# ============================================================================

# Importaci√≥n de las bibliotecas necesarias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import time
import warnings
warnings.filterwarnings('ignore')

# Configurar TensorFlow para mejor rendimiento
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("GPU disponible para aceleraci√≥n")
else:
    print("Ejecutando en CPU")

print("TensorFlow versi√≥n:", tf.__version__)
print("¬øGPU disponible?", tf.config.list_physical_devices('GPU'))

# ============================================================================
# PASO 1: CARGA Y EXPLORACI√ìN DE LOS DATOS
# ============================================================================

print("\n" + "="*60)
print("PASO 1: CARGANDO Y EXPLORANDO LOS DATOS CIFAR-10")
print("="*60)

# CIFAR-10 contiene 60,000 im√°genes de 32x32 en 10 clases
# Las clases son: avi√≥n, autom√≥vil, p√°jaro, gato, ciervo, perro, rana, caballo, barco, cami√≥n
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Nombres de las clases para hacer m√°s interpretables los resultados
nombres_clases = ['Avi√≥n', 'Autom√≥vil', 'P√°jaro', 'Gato', 'Ciervo', 
                  'Perro', 'Rana', 'Caballo', 'Barco', 'Cami√≥n']

print(f"Forma de datos de entrenamiento: {x_train.shape}")
print(f"Forma de etiquetas de entrenamiento: {y_train.shape}")
print(f"Forma de datos de prueba: {x_test.shape}")
print(f"Forma de etiquetas de prueba: {y_test.shape}")
print(f"N√∫mero de clases: {len(nombres_clases)}")

# ============================================================================
# PASO 2: PREPROCESAMIENTO DE DATOS
# ============================================================================

print("\n" + "="*60)
print("PASO 2: PREPROCESAMIENTO DE LOS DATOS")
print("="*60)

# Normalizaci√≥n de p√≠xeles: convertir de rango [0, 255] a [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

print("Datos normalizados al rango [0, 1]")

# Conversi√≥n de etiquetas a codificaci√≥n categ√≥rica (one-hot encoding)
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

print(f"Etiquetas convertidas a formato one-hot")

# Configuraci√≥n simple de data augmentation (muy ligero)
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal")
])

# Configurar tama√±o de batch para equilibrar velocidad y memoria
BATCH_SIZE = 128  # Aumentado para acelerar entrenamiento
VALIDATION_SPLIT = 0.2

# ============================================================================
# PASO 3: CONSTRUCCI√ìN DEL MODELO CNN
# ============================================================================

print("\n" + "="*60)
print("PASO 3: CONSTRUYENDO LA RED NEURONAL CONVOLUCIONAL ANTI-OVERFITTING")
print("="*60)

def crear_modelo_cnn_equilibrado():
    """
    Construye una CNN con fuerte regularizaci√≥n para evitar overfitting
    manteniendo buen rendimiento
    """
    # Capa de entrada con data augmentation b√°sico
    inputs = keras.Input(shape=(32, 32, 3))
    x = data_augmentation(inputs)  # Aplica data augmentation en tiempo real
    
    # Primera capa convolucional
    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)  # Normalizaci√≥n para estabilizar
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.2)(x)  # Dropout moderado temprano
    
    # Segunda capa convolucional
    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.3)(x)  # Aumentando dropout
    
    # Tercera capa convolucional
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Dropout(0.4)(x)  # M√°s dropout en capas profundas
    
    # Capa de clasificaci√≥n con fuerte regularizaci√≥n
    x = layers.Flatten()(x)
    x = layers.Dense(256, activation='relu', 
                    kernel_regularizer=keras.regularizers.l2(0.001))(x)  # L2 regularizaci√≥n
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)  # Dropout intenso en capas finales
    outputs = layers.Dense(10, activation='softmax')(x)
    
    model = keras.Model(inputs, outputs)
    return model

# Crear el modelo
modelo = crear_modelo_cnn_equilibrado()

# Mostrar resumen del modelo
print("Arquitectura del modelo anti-overfitting:")
modelo.summary()

# ============================================================================
# PASO 4: COMPILACI√ìN DEL MODELO
# ============================================================================

print("\n" + "="*60)
print("PASO 4: COMPILANDO EL MODELO")
print("="*60)

# Configuraci√≥n del optimizador con decay
initial_learning_rate = 0.001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.9,
    staircase=True)

optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)

# Compilar con funci√≥n de p√©rdida y m√©tricas
modelo.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("Modelo compilado exitosamente!")
print("Optimizador: Adam con ExponentialDecay (learning_rate=0.001)")
print("Funci√≥n de p√©rdida: categorical_crossentropy")
print("M√©tricas: accuracy")

# ============================================================================
# PASO 5: CONFIGURACI√ìN DE CALLBACKS
# ============================================================================

print("\n" + "="*60)
print("PASO 5: CONFIGURANDO CALLBACKS PARA ENTRENAMIENTO EFICIENTE")
print("="*60)

# Callback para reducir la tasa de aprendizaje cuando el rendimiento se estanca
reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',  # Cambiar a val_loss para enfocarse en generalizaci√≥n
    factor=0.5,
    patience=3,
    min_lr=0.00005,
    verbose=1
)

# Callback para detener el entrenamiento temprano
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',  # Monitorear val_loss para mejor generalizaci√≥n
    patience=5,
    restore_best_weights=True,
    verbose=1
)

# Lista de callbacks
callbacks = [reduce_lr, early_stopping]

print("Callbacks configurados:")
print("- ReduceLROnPlateau: reduce tasa de aprendizaje basado en p√©rdida de validaci√≥n")
print("- EarlyStopping: detiene el entrenamiento cuando la p√©rdida de validaci√≥n deja de mejorar")

# ============================================================================
# PASO 6: ENTRENAMIENTO DEL MODELO
# ============================================================================

print("\n" + "="*60)
print("PASO 6: ENTRENANDO EL MODELO")
print("="*60)

# Configuraci√≥n del entrenamiento
EPOCHS = 30  # M√°s √©pocas para permitir convergencia lenta pero estable

print(f"Configuraci√≥n del entrenamiento:")
print(f"- √âpocas m√°ximas: {EPOCHS}")
print(f"- Tama√±o del lote: {BATCH_SIZE}")
print(f"- Divisi√≥n de validaci√≥n: {VALIDATION_SPLIT}")
print(f"- Data augmentation: Activado (solo horizontal flip)")
print(f"- Regularizaci√≥n: BatchNorm, Dropout progresivo, L2")

print("\nIniciando entrenamiento...")
start_time = time.time()

# Entrenar el modelo
history = modelo.fit(
    x_train, y_train_cat,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=VALIDATION_SPLIT,
    callbacks=callbacks,
    verbose=1
)

end_time = time.time()
training_time = end_time - start_time

print("\n¬°Entrenamiento completado!")
print(f"Tiempo total de entrenamiento: {training_time:.2f} segundos")
print(f"Tiempo promedio por √©poca: {training_time / len(history.history['loss']):.2f} segundos")

# ============================================================================
# PASO 7: VISUALIZACI√ìN DEL ENTRENAMIENTO
# ============================================================================

print("\n" + "="*60)
print("PASO 7: ANALIZANDO EL PROGRESO DEL ENTRENAMIENTO")
print("="*60)

def plot_training_history(history):
    """Visualiza las m√©tricas de entrenamiento a lo largo de las √©pocas"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Gr√°fico de precisi√≥n
    ax1.plot(history.history['accuracy'], label='Precisi√≥n de entrenamiento', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Precisi√≥n de validaci√≥n', linewidth=2)
    ax1.set_title('Evoluci√≥n de la Precisi√≥n del Modelo', fontsize=14)
    ax1.set_xlabel('√âpoca')
    ax1.set_ylabel('Precisi√≥n')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Gr√°fico de p√©rdida
    ax2.plot(history.history['loss'], label='P√©rdida de entrenamiento', linewidth=2)
    ax2.plot(history.history['val_loss'], label='P√©rdida de validaci√≥n', linewidth=2)
    ax2.set_title('Evoluci√≥n de la P√©rdida del Modelo', fontsize=14)
    ax2.set_xlabel('√âpoca')
    ax2.set_ylabel('P√©rdida')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# Mostrar gr√°ficos de entrenamiento
plot_training_history(history)

# Estad√≠sticas finales del entrenamiento
final_train_acc = history.history['accuracy'][-1]
final_val_acc = history.history['val_accuracy'][-1]
overfitting_gap = final_train_acc - final_val_acc

print(f"Resultados finales del entrenamiento:")
print(f"- Precisi√≥n de entrenamiento: {final_train_acc:.4f}")
print(f"- Precisi√≥n de validaci√≥n: {final_val_acc:.4f}")
print(f"- Brecha de overfitting: {overfitting_gap:.4f} ({overfitting_gap*100:.1f}%)")

# ============================================================================
# PASO 8: EVALUACI√ìN EN DATOS DE PRUEBA
# ============================================================================

print("\n" + "="*60)
print("PASO 8: EVALUACI√ìN FINAL EN DATOS DE PRUEBA")
print("="*60)

# Evaluar el modelo en el conjunto de prueba
test_loss, test_accuracy = modelo.evaluate(x_test, y_test_cat, verbose=0)
print(f"Precisi√≥n en datos de prueba: {test_accuracy:.4f}")
print(f"P√©rdida en datos de prueba: {test_loss:.4f}")

# Generar predicciones para an√°lisis detallado
y_pred_probs = modelo.predict(x_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test_cat, axis=1)

# Reporte de clasificaci√≥n detallado
print("\nReporte de clasificaci√≥n detallado:")
print(classification_report(y_true, y_pred, 
                          target_names=nombres_clases, digits=4))

# ============================================================================
# PASO 9: MATRIZ DE CONFUSI√ìN
# ============================================================================

print("\n" + "="*60)
print("PASO 9: AN√ÅLISIS DE ERRORES CON MATRIZ DE CONFUSI√ìN")
print("="*60)

# Calcular matriz de confusi√≥n
cm = confusion_matrix(y_true, y_pred)

# Visualizar matriz de confusi√≥n
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=nombres_clases, yticklabels=nombres_clases)
plt.title('Matriz de Confusi√≥n - Resultados de Clasificaci√≥n', fontsize=16)
plt.xlabel('Predicciones')
plt.ylabel('Valores Reales')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# An√°lisis de precisi√≥n por clase
precisiones_por_clase = cm.diagonal() / cm.sum(axis=1)
print("Precisi√≥n por clase:")
for i, nombre in enumerate(nombres_clases):
    print(f"{nombre}: {precisiones_por_clase[i]:.4f}")

# ============================================================================
# PASO 10: RESUMEN Y CONCLUSIONES
# ============================================================================

print("\n" + "="*60)
print("PASO 10: RESUMEN Y CONCLUSIONES")
print("="*60)

print("üéâ ¬°DEMOSTRACI√ìN COMPLETADA EXITOSAMENTE!")
print("\nResumen de resultados del modelo anti-overfitting:")
print(f"‚úÖ Precisi√≥n final en datos de prueba: {test_accuracy:.1%}")
print(f"‚úÖ Tiempo total de entrenamiento: {training_time:.2f} segundos")
print(f"‚úÖ Tiempo promedio por √©poca: {training_time / len(history.history['loss']):.2f} segundos")
print(f"‚úÖ N√∫mero de par√°metros del modelo: {modelo.count_params():,}")
print(f"‚úÖ Entrenamiento completado en {len(history.history['loss'])} √©pocas")
print(f"‚úÖ Brecha de overfitting: {overfitting_gap:.1%}")

print("\nT√©cnicas aplicadas contra el overfitting:")
print("‚Ä¢ Regularizaci√≥n L2 en capas densas")
print("‚Ä¢ Dropout progresivo (0.2 ‚Üí 0.3 ‚Üí 0.4 ‚Üí 0.5)")
print("‚Ä¢ BatchNormalization en cada bloque convolucional")
print("‚Ä¢ Data augmentation en tiempo real (horizontal flip)")
print("‚Ä¢ Learning rate con decay exponencial")
print("‚Ä¢ Early stopping basado en p√©rdida de validaci√≥n")

print("\n" + "="*60)
print("¬°Gracias por explorar las redes neuronales convolucionales!")
print("="*60)