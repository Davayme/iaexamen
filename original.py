# ============================================================================
# DEMOSTRACI√ìN DE REDES NEURONALES CONVOLUCIONALES CON CIFAR-10
# Programa educativo para Google Colab
# ============================================================================

# Importaci√≥n de las bibliotecas necesarias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

print("TensorFlow versi√≥n:", tf.__version__)
print("¬øGPU disponible?", tf.config.list_physical_devices('GPU'))

# ============================================================================
# PASO 1: CARGA Y EXPLORACI√ìN DE LOS DATOS
# ============================================================================

print("\n" + "="*60)
print("PASO 1: CARGANDO Y EXPLORANDO LOS DATOS CIFAR-10")
print("="*60)

# CIFAR-10 contiene 60,000 im√°genes de 32x32 en 10 clases
# Las clases son: avi√≥n, autom√≥vil, p√°jaro, gato, ciervo, perro, rana, caballo, barco, cami√≥n
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Nombres de las clases para hacer m√°s interpretables los resultados
nombres_clases = ['Avi√≥n', 'Autom√≥vil', 'P√°jaro', 'Gato', 'Ciervo', 
                  'Perro', 'Rana', 'Caballo', 'Barco', 'Cami√≥n']

print(f"Forma de datos de entrenamiento: {x_train.shape}")
print(f"Forma de etiquetas de entrenamiento: {y_train.shape}")
print(f"Forma de datos de prueba: {x_test.shape}")
print(f"Forma de etiquetas de prueba: {y_test.shape}")
print(f"N√∫mero de clases: {len(nombres_clases)}")

# Visualizaci√≥n de algunas im√°genes de ejemplo
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
fig.suptitle('Ejemplos de im√°genes CIFAR-10', fontsize=16)

for i in range(10):
    fila = i // 5
    col = i % 5
    axes[fila, col].imshow(x_train[i])
    axes[fila, col].set_title(f'{nombres_clases[y_train[i][0]]}')
    axes[fila, col].axis('off')

plt.tight_layout()
plt.show()

# An√°lisis de la distribuci√≥n de clases
plt.figure(figsize=(10, 6))
clases_unicas, conteos = np.unique(y_train, return_counts=True)
plt.bar([nombres_clases[i] for i in clases_unicas], conteos, color='skyblue')
plt.title('Distribuci√≥n de clases en el conjunto de entrenamiento')
plt.xlabel('Clases')
plt.ylabel('N√∫mero de im√°genes')
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# ============================================================================
# PASO 2: PREPROCESAMIENTO DE DATOS
# ============================================================================

print("\n" + "="*60)
print("PASO 2: PREPROCESAMIENTO DE LOS DATOS")
print("="*60)

# Normalizaci√≥n de p√≠xeles: convertir de rango [0, 255] a [0, 1]
# Esto ayuda a que la red neuronal converja m√°s r√°pidamente
x_train_norm = x_train.astype('float32') / 255.0
x_test_norm = x_test.astype('float32') / 255.0

print("Antes de normalizaci√≥n - Rango de p√≠xeles:", x_train.min(), "a", x_train.max())
print("Despu√©s de normalizaci√≥n - Rango de p√≠xeles:", x_train_norm.min(), "a", x_train_norm.max())

# Conversi√≥n de etiquetas a codificaci√≥n categ√≥rica (one-hot encoding)
# Esto transforma etiquetas como [3] a [0,0,0,1,0,0,0,0,0,0]
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

print(f"Forma de etiquetas antes: {y_train.shape}")
print(f"Forma de etiquetas despu√©s: {y_train_cat.shape}")
print(f"Ejemplo de etiqueta original: {y_train[0]} -> Categ√≥rica: {y_train_cat[0]}")

# ============================================================================
# PASO 3: CONSTRUCCI√ìN DEL MODELO CNN
# ============================================================================

print("\n" + "="*60)
print("PASO 3: CONSTRUYENDO LA RED NEURONAL CONVOLUCIONAL")
print("="*60)

def crear_modelo_cnn():
    """
    Construye una CNN con arquitectura moderna y t√©cnicas de regularizaci√≥n.
    
    La arquitectura incluye:
    - Capas convolucionales para extracci√≥n de caracter√≠sticas
    - Capas de pooling para reducir dimensionalidad
    - Batch normalization para estabilizar el entrenamiento
    - Dropout para prevenir sobreajuste
    - Capas densas para clasificaci√≥n final
    """
    modelo = keras.Sequential([
        # Primer bloque convolucional
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Segundo bloque convolucional
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Tercer bloque convolucional
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.25),
        
        # Capas de clasificaci√≥n
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')  # 10 clases de salida
    ])
    
    return modelo

# Crear el modelo
modelo = crear_modelo_cnn()

# Mostrar la arquitectura del modelo
print("Arquitectura del modelo:")
modelo.summary()

# Visualizar la arquitectura
tf.keras.utils.plot_model(modelo, show_shapes=True, show_layer_names=True, 
                          rankdir="TB", dpi=150)

# ============================================================================
# PASO 4: COMPILACI√ìN DEL MODELO
# ============================================================================

print("\n" + "="*60)
print("PASO 4: COMPILANDO EL MODELO")
print("="*60)

# Configuraci√≥n del optimizador y funciones de p√©rdida
modelo.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizador Adam con tasa de aprendizaje
    loss='categorical_crossentropy',                        # Funci√≥n de p√©rdida para clasificaci√≥n multiclase
    metrics=['accuracy']                                    # M√©trica para monitorear durante el entrenamiento
)

print("Modelo compilado exitosamente!")
print("Optimizador: Adam (learning_rate=0.001)")
print("Funci√≥n de p√©rdida: categorical_crossentropy")
print("M√©tricas: accuracy")

# ============================================================================
# PASO 5: CONFIGURACI√ìN DE CALLBACKS
# ============================================================================

print("\n" + "="*60)
print("PASO 5: CONFIGURANDO CALLBACKS PARA EL ENTRENAMIENTO")
print("="*60)

# Callback para reducir la tasa de aprendizaje cuando el rendimiento se estanque
reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=5,
    min_lr=0.0001,
    verbose=1
)

# Callback para detener el entrenamiento temprano si no hay mejora
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

callbacks = [reduce_lr, early_stopping]
print("Callbacks configurados:")
print("- ReduceLROnPlateau: reduce tasa de aprendizaje si no hay mejora")
print("- EarlyStopping: detiene entrenamiento temprano para evitar sobreajuste")

# ============================================================================
# PASO 6: ENTRENAMIENTO DEL MODELO
# ============================================================================

print("\n" + "="*60)
print("PASO 6: ENTRENANDO EL MODELO")
print("="*60)

# Configuraci√≥n del entrenamiento
EPOCHS = 50          # N√∫mero m√°ximo de √©pocas
BATCH_SIZE = 32      # Tama√±o del lote para entrenamiento
VALIDATION_SPLIT = 0.2  # Porcentaje de datos para validaci√≥n

print(f"Configuraci√≥n del entrenamiento:")
print(f"- √âpocas m√°ximas: {EPOCHS}")
print(f"- Tama√±o del lote: {BATCH_SIZE}")
print(f"- Divisi√≥n de validaci√≥n: {VALIDATION_SPLIT}")
print(f"- Datos de entrenamiento: {len(x_train_norm) * (1-VALIDATION_SPLIT):.0f} im√°genes")
print(f"- Datos de validaci√≥n: {len(x_train_norm) * VALIDATION_SPLIT:.0f} im√°genes")

print("\nIniciando entrenamiento...")

# Entrenar el modelo
history = modelo.fit(
    x_train_norm, y_train_cat,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=VALIDATION_SPLIT,
    callbacks=callbacks,
    verbose=1
)

print("\n¬°Entrenamiento completado!")

# ============================================================================
# PASO 7: VISUALIZACI√ìN DEL ENTRENAMIENTO
# ============================================================================

print("\n" + "="*60)
print("PASO 7: ANALIZANDO EL PROGRESO DEL ENTRENAMIENTO")
print("="*60)

def plot_training_history(history):
    """Visualiza las m√©tricas de entrenamiento a lo largo de las √©pocas"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Gr√°fico de precisi√≥n
    ax1.plot(history.history['accuracy'], label='Precisi√≥n de entrenamiento', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Precisi√≥n de validaci√≥n', linewidth=2)
    ax1.set_title('Evoluci√≥n de la Precisi√≥n del Modelo', fontsize=14)
    ax1.set_xlabel('√âpoca')
    ax1.set_ylabel('Precisi√≥n')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Gr√°fico de p√©rdida
    ax2.plot(history.history['loss'], label='P√©rdida de entrenamiento', linewidth=2)
    ax2.plot(history.history['val_loss'], label='P√©rdida de validaci√≥n', linewidth=2)
    ax2.set_title('Evoluci√≥n de la P√©rdida del Modelo', fontsize=14)
    ax2.set_xlabel('√âpoca')
    ax2.set_ylabel('P√©rdida')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# Mostrar gr√°ficos de entrenamiento
plot_training_history(history)

# Estad√≠sticas finales del entrenamiento
final_train_acc = history.history['accuracy'][-1]
final_val_acc = history.history['val_accuracy'][-1]
final_train_loss = history.history['loss'][-1]
final_val_loss = history.history['val_loss'][-1]

print(f"Resultados finales del entrenamiento:")
print(f"- Precisi√≥n de entrenamiento: {final_train_acc:.4f}")
print(f"- Precisi√≥n de validaci√≥n: {final_val_acc:.4f}")
print(f"- P√©rdida de entrenamiento: {final_train_loss:.4f}")
print(f"- P√©rdida de validaci√≥n: {final_val_loss:.4f}")

# ============================================================================
# PASO 8: EVALUACI√ìN EN DATOS DE PRUEBA
# ============================================================================

print("\n" + "="*60)
print("PASO 8: EVALUACI√ìN FINAL EN DATOS DE PRUEBA")
print("="*60)

# Evaluar el modelo en el conjunto de prueba
test_loss, test_accuracy = modelo.evaluate(x_test_norm, y_test_cat, verbose=0)
print(f"Precisi√≥n en datos de prueba: {test_accuracy:.4f}")
print(f"P√©rdida en datos de prueba: {test_loss:.4f}")

# Generar predicciones para an√°lisis detallado
y_pred = modelo.predict(x_test_norm)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test_cat, axis=1)

# Reporte de clasificaci√≥n detallado
print("\nReporte de clasificaci√≥n detallado:")
print(classification_report(y_true_classes, y_pred_classes, 
                          target_names=nombres_clases, digits=4))

# ============================================================================
# PASO 9: MATRIZ DE CONFUSI√ìN
# ============================================================================

print("\n" + "="*60)
print("PASO 9: AN√ÅLISIS DE ERRORES CON MATRIZ DE CONFUSI√ìN")
print("="*60)

# Calcular matriz de confusi√≥n
cm = confusion_matrix(y_true_classes, y_pred_classes)

# Visualizar matriz de confusi√≥n
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=nombres_clases, yticklabels=nombres_clases)
plt.title('Matriz de Confusi√≥n - Resultados de Clasificaci√≥n', fontsize=16)
plt.xlabel('Predicciones')
plt.ylabel('Valores Reales')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# An√°lisis de precisi√≥n por clase
precisiones_por_clase = cm.diagonal() / cm.sum(axis=1)
print("Precisi√≥n por clase:")
for i, nombre in enumerate(nombres_clases):
    print(f"{nombre}: {precisiones_por_clase[i]:.4f}")

# ============================================================================
# PASO 10: EJEMPLOS DE PREDICCIONES
# ============================================================================

print("\n" + "="*60)
print("PASO 10: VISUALIZACI√ìN DE PREDICCIONES INDIVIDUALES")
print("="*60)

def mostrar_predicciones(modelo, x_test, y_test, nombres_clases, num_ejemplos=12):
    """Muestra ejemplos de predicciones con sus probabilidades"""
    
    # Seleccionar ejemplos aleatorios
    indices = np.random.choice(len(x_test), num_ejemplos, replace=False)
    
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    fig.suptitle('Ejemplos de Predicciones del Modelo', fontsize=16)
    
    for i, idx in enumerate(indices):
        fila = i // 4
        col = i % 4
        
        # Realizar predicci√≥n
        pred = modelo.predict(x_test[idx:idx+1])
        pred_class = np.argmax(pred[0])
        pred_prob = np.max(pred[0])
        true_class = y_test[idx][0]
        
        # Mostrar imagen
        axes[fila, col].imshow(x_test[idx])
        
        # Color del t√≠tulo seg√∫n si la predicci√≥n es correcta
        color = 'green' if pred_class == true_class else 'red'
        
        axes[fila, col].set_title(
            f'Real: {nombres_clases[true_class]}\n'
            f'Pred: {nombres_clases[pred_class]} ({pred_prob:.2f})',
            color=color, fontsize=10
        )
        axes[fila, col].axis('off')
    
    plt.tight_layout()
    plt.show()

# Mostrar ejemplos de predicciones
mostrar_predicciones(modelo, x_test, y_test, nombres_clases)

# ============================================================================
# PASO 11: AN√ÅLISIS DE CARACTER√çSTICAS APRENDIDAS
# ============================================================================

print("\n" + "="*60)
print("PASO 11: VISUALIZACI√ìN DE FILTROS CONVOLUCIONALES")
print("="*60)

def visualizar_filtros(modelo, capa_idx=0, num_filtros=8):
    """Visualiza los filtros aprendidos por una capa convolucional"""
    
    # Obtener los pesos de la primera capa convolucional
    filtros = modelo.layers[capa_idx].get_weights()[0]
    
    # Normalizar filtros para visualizaci√≥n
    f_min, f_max = filtros.min(), filtros.max()
    filtros = (filtros - f_min) / (f_max - f_min)
    
    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    fig.suptitle(f'Filtros de la Capa Convolucional {capa_idx + 1}', fontsize=14)
    
    for i in range(min(num_filtros, filtros.shape[3])):
        fila = i // 4
        col = i % 4
        axes[fila, col].imshow(filtros[:, :, :, i])
        axes[fila, col].set_title(f'Filtro {i+1}')
        axes[fila, col].axis('off')
    
    plt.tight_layout()
    plt.show()

# Visualizar filtros de la primera capa
visualizar_filtros(modelo, capa_idx=0)

# ============================================================================
# PASO 12: RESUMEN Y CONCLUSIONES
# ============================================================================

print("\n" + "="*60)
print("PASO 12: RESUMEN Y CONCLUSIONES")
print("="*60)

print("üéâ ¬°DEMOSTRACI√ìN COMPLETADA EXITOSAMENTE!")
print("\nResumen de resultados:")
print(f"‚úÖ Modelo entrenado con {len(x_train)} im√°genes")
print(f"‚úÖ Precisi√≥n final en prueba: {test_accuracy:.1%}")
print(f"‚úÖ Arquitectura CNN con {modelo.count_params():,} par√°metros")
print(f"‚úÖ Entrenamiento completado en {len(history.history['loss'])} √©pocas")

print("\nüìö Conceptos demostrados:")
print("‚Ä¢ Carga y preprocesamiento de datos de imagen")
print("‚Ä¢ Construcci√≥n de arquitectura CNN moderna")
print("‚Ä¢ T√©cnicas de regularizaci√≥n (Dropout, BatchNorm)")
print("‚Ä¢ Entrenamiento con callbacks inteligentes")
print("‚Ä¢ Evaluaci√≥n exhaustiva de resultados")
print("‚Ä¢ Visualizaci√≥n de filtros y predicciones")

print("\nüîç Puntos clave aprendidos:")
print("‚Ä¢ Las CNNs extraen caracter√≠sticas jer√°rquicas autom√°ticamente")
print("‚Ä¢ La normalizaci√≥n de datos mejora la convergencia")
print("‚Ä¢ Los callbacks previenen el sobreajuste efectivamente")
print("‚Ä¢ La matriz de confusi√≥n revela patrones de error espec√≠ficos")
print("‚Ä¢ Los filtros convolucionales aprenden detectores de caracter√≠sticas")

print("\nüöÄ Pr√≥ximos pasos sugeridos:")
print("‚Ä¢ Experimentar con diferentes arquitecturas (ResNet, VGG)")
print("‚Ä¢ Probar t√©cnicas de aumento de datos (data augmentation)")
print("‚Ä¢ Implementar transfer learning con modelos preentrenados")
print("‚Ä¢ Explorar otros conjuntos de datos (CIFAR-100, ImageNet)")

print("\n" + "="*60)
print("¬°Gracias por explorar las redes neuronales convolucionales!")
print("="*60)